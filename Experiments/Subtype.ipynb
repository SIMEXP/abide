{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtype analysis\n",
    "This experiment aims to identify subtypes within the ABIDE dataset based on three different types of maps:\n",
    "\n",
    "1. Scores\n",
    "2. Dual Regression\n",
    "3. Seed Functional Connectivity\n",
    "\n",
    "I will then compute individual weights for each of these subjects reflecting the overall similarity of an individuals map with the average subtype map. These weights will then be used in a GLM to regress against a number of phenotype variables.\n",
    "\n",
    "## Scientific Assumptions\n",
    "\n",
    "1. We will identify 7 subtypes per map. This could also be a data driven number, but not for now.\n",
    "\n",
    "## Scientific Question\n",
    "These are the questions I want to ask:\n",
    "\n",
    "1. What are the subtype maps and how do they differ within a map-type\n",
    "2. Are there subtypes for which the individual weights are predictive of the phenotype data\n",
    "3. Is there a map type that is more useful for this investigation than the others (or a ranking)\n",
    "\n",
    "## Practical Questions\n",
    "\n",
    "1. Do we really need to demean the maps? For most of the maps, the values are already scaled. They are not 0-centered, but that could very well be meaningful. I'm not sure what demeaning does then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats as st\n",
    "from scipy import cluster as scl\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model as slin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "scale = 7\n",
    "subtypes = 7\n",
    "network = 1\n",
    "net_id = network - 1\n",
    "template = '*_fmri_{:07d}_session_1_run1_stability_maps.nii.gz'\n",
    "data_path = '/data1/abide/Out/Scores/sc{:02d}/time'.format(scale)\n",
    "pheno_path = '/data1/abide/Pheno/pheno.csv'\n",
    "mask_path = '/data1/abide/Mask/mask_data_specific.nii.gz'\n",
    "map_types = ['stability_maps', 'rmap_part', 'dual_regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the mask\n",
    "m_img = nib.load(mask_path)\n",
    "mask = m_img.get_data()\n",
    "mask = mask!=0\n",
    "n_vox = np.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the phenotype information\n",
    "pheno = pd.read_csv(pheno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through the subject ID's and find the corresponding \n",
    "# files. If there is no file, drop the subject\n",
    "drop_id = list()\n",
    "path_list = list()\n",
    "for index, row in pheno.iterrows():\n",
    "    s_id = row['SUB_ID']\n",
    "    s_path = glob.glob(os.path.join(data_path, map_types[0], template.format(s_id)))\n",
    "    if s_path:\n",
    "        path_list.append(s_path[0])\n",
    "    else:\n",
    "        drop_id.append(index)\n",
    "        continue\n",
    "clean_pheno = pheno.drop(drop_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_files = len(path_list)\n",
    "# Prepare the storage matrix\n",
    "net_mat = np.zeros((n_vox, n_files))\n",
    "# Go through the files\n",
    "for index, s_path in enumerate(path_list):\n",
    "    f_net = nib.load(s_path).get_data()[mask][..., net_id]\n",
    "    net_mat[..., index] = f_net\n",
    "net_mat = net_mat - np.mean(net_mat,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a correlation matrix of the subjects\n",
    "corr_sub = np.corrcoef(net_mat, rowvar=0)\n",
    "link_sub = scl.hierarchy.ward(corr_sub)\n",
    "part_sub = scl.hierarchy.fcluster(link_sub, subtypes, criterion='maxclust')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the average of the subtypes\n",
    "sbt_avg = np.zeros((n_vox, subtypes))\n",
    "for idx in range(subtypes):\n",
    "    sub_id = np.unique(part_sub)[idx]\n",
    "    sbt_avg[..., idx] = np.mean(net_mat[...,part_sub==sub_id],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the individual weights\n",
    "y_stp = np.zeros((n_files, subtypes))\n",
    "for s_id in range(subtypes):\n",
    "    type_map = sbt_avg[:, s_id]\n",
    "    y_stp[:, s_id] = np.array([np.corrcoef(type_map, net_mat[:,x])[0,1] for x in range(n_files)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the features from the phenotype data\n",
    "X_diag = pd.get_dummies(clean_pheno['SITE_ID'])\n",
    "#X_diag = X_diag.rename(columns={'CALTECH': 'INTERCEPT'})\n",
    "#X_diag['INTERCEPT'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.178\n",
      "Model:                            OLS   Adj. R-squared:                  0.163\n",
      "Method:                 Least Squares   F-statistic:                     11.98\n",
      "Date:                Fri, 03 Jul 2015   Prob (F-statistic):           8.99e-29\n",
      "Time:                        15:59:25   Log-Likelihood:                 903.54\n",
      "No. Observations:                 901   AIC:                            -1773.\n",
      "Df Residuals:                     884   BIC:                            -1691.\n",
      "Df Model:                          16                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "CALTECH        0.6884      0.015     47.350      0.000         0.660     0.717\n",
      "CMU            0.6437      0.017     37.326      0.000         0.610     0.678\n",
      "KKI            0.7081      0.012     58.603      0.000         0.684     0.732\n",
      "LEUVEN_1       0.7470      0.017     44.889      0.000         0.714     0.780\n",
      "LEUVEN_2       0.8001      0.015     52.820      0.000         0.770     0.830\n",
      "MAX_MUN        0.6543      0.012     55.124      0.000         0.631     0.678\n",
      "NYU            0.6976      0.007    105.598      0.000         0.685     0.711\n",
      "OHSU           0.6658      0.017     39.311      0.000         0.633     0.699\n",
      "OLIN           0.6912      0.015     46.276      0.000         0.662     0.720\n",
      "PITT           0.7050      0.012     59.397      0.000         0.682     0.728\n",
      "SBL            0.6732      0.016     41.143      0.000         0.641     0.705\n",
      "SDSU           0.7826      0.015     52.398      0.000         0.753     0.812\n",
      "STANFORD       0.7408      0.014     52.283      0.000         0.713     0.769\n",
      "TRINITY        0.7697      0.013     60.125      0.000         0.745     0.795\n",
      "UCLA_1         0.6638      0.010     63.289      0.000         0.643     0.684\n",
      "UCLA_2         0.6384      0.018     36.322      0.000         0.604     0.673\n",
      "USM            0.7385      0.009     82.814      0.000         0.721     0.756\n",
      "==============================================================================\n",
      "Omnibus:                      162.389   Durbin-Watson:                   1.978\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              329.996\n",
      "Skew:                          -1.030   Prob(JB):                     2.20e-72\n",
      "Kurtosis:                       5.132   Cond. No.                         2.66\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y_stp[:,2], X_diag)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.identity(len(results.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = A[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = results.f_test(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<F test: F=array([[ 3375.42314973]]), p=0.0, df_denom=884, df_num=16>\n"
     ]
    }
   ],
   "source": [
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
